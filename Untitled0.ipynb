{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNUVo0WHIRjwsH/Yo7cCtO9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rushipanchal2/Pandas/blob/main/Untitled0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "id": "_KsYwEQV4eqe",
        "outputId": "7d97ffc0-4fbb-479a-ade3-12f0dfd25feb"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-21d8fbfa1d9a>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    Run the Cell to import the packages\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "Run the Cell to import the packages\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import csv\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import csv\n",
        "Fill in the Command to load your CSV dataset \"imdb.csv\" with pandas\n",
        "\n",
        "\"]\n",
        "#Data Loading\n",
        "imdb=pd.read_csv(\"imdb.csv\"      )\n",
        "imdb.columns = [\"index\",\"text\",\"label\"]\n",
        "print(imdb.head(5))\n",
        "Data Analysis\n",
        "\n",
        "Get the shape of the dataset and print it.\n",
        "\n",
        "Get the column names in list and print it.\n",
        "\n",
        "Group the dataset by label and describe the dataset to understand the basic statistics of the dataset.\n",
        "\n",
        "Print the first three rows of the dataset\n",
        "\n",
        "data_size =imdb.shape\n",
        "​\n",
        "print(data_size)\n",
        "​\n",
        "imdb_col_names =imdb.columns\n",
        "​\n",
        "print(imdb_col_names)\n",
        "print(imdb.groupby('label').describe()          )\n",
        "print( imdb.head(3)          )\n",
        "Target Identification\n",
        "\n",
        "Execute the below cell to identify the target variables. If 0 it is a bad review,if it is 1 it is a good review.\n",
        "\n",
        "imdb_target=imdb['label'] \n",
        "​\n",
        "print(imdb_target)\n",
        "​\n",
        "Tokenization\n",
        "\n",
        "Convert the text into lower.\n",
        "Tokenize the text using word_tokenize\n",
        "Apply the function split_tokens for the column text in the imdb dataset with axis =1\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('all')\n",
        "​\n",
        "​\n",
        "def split_tokens(text):\n",
        "​\n",
        "  message = text.lower() \n",
        "  \n",
        "    \n",
        "  word_tokens =word_tokenize(message) \n",
        "​\n",
        "  return word_tokens\n",
        "​\n",
        "imdb['tokenized_message'] = imdb.apply(lambda row: split_tokens(row['text']),axis=1)\n",
        "Lemmatization\n",
        "\n",
        "Apply the function split_into_lemmas for the column tokenized_message with axis=1\n",
        "Print the 55th row from the column tokenized_message.\n",
        "Print the 55th row from the column lemmatized_message\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "​\n",
        "def split_into_lemmas(text):\n",
        "​\n",
        "    lemma = []\n",
        "​\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "​\n",
        "    for word in text:\n",
        "​\n",
        "        a=lemmatizer.lemmatize(word)\n",
        "​\n",
        "        lemma.append(a)\n",
        "​\n",
        "    return lemma\n",
        "​\n",
        " \n",
        "​\n",
        "imdb['lemmatized_message'] = imdb.apply(lambda row:split_into_lemmas(row['tokenized_message']),axis=1)\n",
        "​\n",
        "​\n",
        "​\n",
        "print('Tokenized message:', imdb['tokenized_message'][11]                       )\n",
        "​\n",
        "print('Lemmatized message:', imdb['lemmatized_message']                     )\n",
        "Stop Word Removal\n",
        "\n",
        "Set the stop words language as english in the variable stop_words\n",
        "Apply the function stopword_removal to the column lemmatized_message with axis=1\n",
        "Print the 55th row from the column preprocessed_message\n",
        "from nltk.corpus import stopwords\n",
        "​\n",
        "​\n",
        "​\n",
        "def stopword_removal(text):\n",
        "​\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "​\n",
        "    filtered_sentence = []\n",
        "​\n",
        "    filtered_sentence = ' '.join([word for word in text if word not in stop_words])\n",
        "​\n",
        "    return filtered_sentence\n",
        "​\n",
        "​\n",
        "​\n",
        "imdb['preprocessed_message'] = imdb.apply(lambda row: stopword_removal(row['lemmatized_message']),axis=1)\n",
        "​\n",
        "print('Preprocessed message:',imdb['preprocessed_message'][55])\n",
        "​\n",
        "Training_data=pd.Series(list(imdb['preprocessed_message']))\n",
        "​\n",
        "Training_label=pd.Series(list(imdb['label']))\n",
        "​\n",
        "Term Document Matrix\n",
        "\n",
        "Apply CountVectorizer with following parameters\n",
        "ngram_range = (1,2)\n",
        "min_df = (1/len(Training_label))\n",
        "max_df = 0.7\n",
        "Fit the tf_vectorizer with the Training_data\n",
        "Transform the Total_Dictionary_TDM with the Training_data\n",
        "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
        "​\n",
        "tf_vectorizer = CountVectorizer(                                        )\n",
        "​\n",
        "Total_Dictionary_TDM = tf_vectorizer.fit(Training_data)\n",
        "​\n",
        "message_data_TDM = Total_Dictionary_TDM.transform(Training_data)\n",
        "​\n",
        "Term Frequency Inverse Document Frequency (TFIDF)\n",
        "\n",
        "Apply TfidfVectorizer with following parameters\n",
        "ngram_range = (1,2)\n",
        "min_df = (1/len(Training_label))\n",
        "max_df = 0.7\n",
        "Fit the tfidf_vectorizer with the Training_data\n",
        "Transform the Total_Dictionary_TFIDF with the Training_data\n",
        "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
        "​\n",
        "tfidf_vectorizer = TfidfVectorizer(                          )\n",
        "​\n",
        "Total_Dictionary_TFIDF = tfidf_vectorizer.fit(Training_data) \n",
        "​\n",
        "message_data_TFIDF = Total_Dictionary_TFIDF.transform(Training_data) \n",
        "​\n",
        "Train and Test Data\n",
        "\n",
        "Splitting the data for training and testing(90% train,10% test)\n",
        "\n",
        "Perform train-test split on message_data_TDM and Training_label with 90% as train data and 10% as test data.\n",
        "from sklearn.model_selection import train_test_split#Splitting the data for training and testing\n",
        "​\n",
        "train_data,test_data, train_label, test_label = train_test_split(message_data_TDM, Training_label, test_size=.1)\n",
        "Support Vector Machine\n",
        "\n",
        "Get the shape of the train-data and print the same.\n",
        "\n",
        "Get the shape of the test-data and print the same.\n",
        "\n",
        "Initialize SVM classifier with following parameters\n",
        "\n",
        "kernel = linear\n",
        "C= 0.025\n",
        "random_state=seed\n",
        "Train the model with train_data and train_label\n",
        "\n",
        "Now predict the output with test_data\n",
        "\n",
        "Evaluate the classifier with score from test_data and test_label\n",
        "\n",
        "Print the predicted score\n",
        "\n",
        "seed=9\n",
        "from sklearn.svm import SVC\n",
        "​\n",
        "train_data_shape =train_data.shape\n",
        "​\n",
        "test_data_shape = test_data.shape\n",
        "print(\"The shape of train data\" ,train_data_shape           )\n",
        "​\n",
        "print(\"The shape of test data\", test_data_shape            )\n",
        "​\n",
        "classifier = SVC(kernel=\"linear\", C=0.025,random_state=seed)                                   \n",
        "​\n",
        "classifier =classifier.fit(train_data, train_label) \n",
        "​\n",
        "target = classifier.predict(test_data)\n",
        "​\n",
        "score = classifier.score(test_data, test_label)\n",
        "​\n",
        "print('SVM Classifier : ',score)\n",
        "​\n",
        "​\n",
        "with open('output.txt', 'w') as file:\n",
        "    file.write(str((imdb['tokenized_message'][55],imdb['lemmatized_message'][55])))\n",
        "Stochastic Gradient Descent Classifier\n",
        "\n",
        "Perform train-test split on message_data_TDM and Training_label with this time 80% as train data and 20% as test data.\n",
        "\n",
        "Get the shape of the train-data and print the same.\n",
        "\n",
        "Get the shape of the test-data and print the same.\n",
        "\n",
        "Initialize SVM classifier with following parameters\n",
        "\n",
        "loss = modified_huber\n",
        "shuffle= True\n",
        "random_state=seed\n",
        "Train the model with train_data and train_label\n",
        "\n",
        "Now predict the output with test_data\n",
        "\n",
        "Evaluate the classifier with score from test_data and test_label\n",
        "\n",
        "Print the predicted score\n",
        "\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "\n",
        "train_data,test_data, train_label, test_label = train_test_split(message_data_TDM, Training_label, test_size=.2)\n",
        "\n",
        "train_data_shape =train_data.shape\n",
        "\n",
        "test_data_shape = test_data.shape \n",
        "\n",
        "print(\"The shape of train data\", train_data_shape           )\n",
        "\n",
        "print(\"The shape of test data\", test_data_shape           )\n",
        "\n",
        "classifier =  SGDClassifier(loss='modified_huber', shuffle=True,random_state=seed)                              \n",
        "\n",
        "classifier =classifier.fit(train_data, train_label)  \n",
        "\n",
        "target=classifier.predict(test_data)\n",
        "\n",
        "score = classifier.score(test_data, test_label)\n",
        "\n",
        "print('SGD classifier : ',score)\n",
        "\n",
        "with open('output1.txt', 'w') as file:\n",
        "    file.write(str((imdb['preprocessed_message'][55])))\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "​\n",
        "train_data,test_data, train_label, test_label = train_test_split(message_data_TDM, Training_label, test_size=.2)\n",
        "​\n",
        "train_data_shape =train_data.shape\n",
        "​\n",
        "test_data_shape = test_data.shape \n",
        "​\n",
        "print(\"The shape of train data\", train_data_shape           )\n",
        "​\n",
        "print(\"The shape of test data\", test_data_shape           )\n",
        "​\n",
        "classifier =  SGDClassifier(loss='modified_huber', shuffle=True,random_state=seed)                              \n",
        "​\n",
        "classifier =classifier.fit(train_data, train_label)  \n",
        "​\n",
        "target=classifier.predict(test_data)\n",
        "​\n",
        "score = classifier.score(test_data, test_label)\n",
        "​\n",
        "print('SGD classifier : ',score)\n",
        "​\n",
        "with open('output1.txt', 'w') as file:\n",
        "    file.write(str((imdb['preprocessed_message'][55])))"
      ]
    }
  ]
}